def @main(%input_1_int8: Tensor[(1, 32, 32, 3), int8] /* ty=Tensor[(1, 32, 32, 3), int8] span=input_1_int8:0:0 */, %v_param_1: Tensor[(3, 3, 3, 16), int8] /* ty=Tensor[(3, 3, 3, 16), int8] span=model/conv2d/Conv2D:0:0 */, %v_param_2: Tensor[(16), int32] /* ty=Tensor[(16), int32] span=model/activation/Relu;model/batch_normalization/FusedBatchNormV3;model/conv2d/BiasAdd/ReadVariableOp/resource;model/conv2d/BiasAdd;model/conv2d_2/Conv2D;model/conv2d/Conv2D:0:0 */, %v_param_3: Tensor[(3, 3, 16, 16), int8] /* ty=Tensor[(3, 3, 16, 16), int8] span=model/conv2d_1/Conv2D:0:0 */, %v_param_4: Tensor[(16), int32] /* ty=Tensor[(16), int32] span=model/activation_1/Relu;model/batch_normalization_1/FusedBatchNormV3;model/conv2d_1/BiasAdd/ReadVariableOp/resource;model/conv2d_1/BiasAdd;model/conv2d_2/Conv2D;model/conv2d_1/Conv2D:0:0 */, %v_param_5: Tensor[(3, 3, 16, 16), int8] /* ty=Tensor[(3, 3, 16, 16), int8] span=model/conv2d_2/Conv2D:0:0 */, %v_param_6: Tensor[(16), int32] /* ty=Tensor[(16), int32] span=model/batch_normalization_2/FusedBatchNormV3;model/conv2d_2/BiasAdd/ReadVariableOp/resource;model/conv2d_2/BiasAdd:0:0 */, %v_param_11: Tensor[(1, 1, 16, 32), int8] /* ty=Tensor[(1, 1, 16, 32), int8] span=model/conv2d_5/Conv2D:0:0 */, %v_param_12: Tensor[(32), int32] /* ty=Tensor[(32), int32] span=model/conv2d_5/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_5/BiasAdd/ReadVariableOp/resource:0:0 */, %v_param_7: Tensor[(3, 3, 16, 32), int8] /* ty=Tensor[(3, 3, 16, 32), int8] span=model/conv2d_3/Conv2D:0:0 */, %v_param_8: Tensor[(32), int32] /* ty=Tensor[(32), int32] span=model/activation_3/Relu;model/batch_normalization_3/FusedBatchNormV3;model/conv2d_3/BiasAdd/ReadVariableOp/resource;model/conv2d_3/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_3/Conv2D:0:0 */, %v_param_9: Tensor[(3, 3, 32, 32), int8] /* ty=Tensor[(3, 3, 32, 32), int8] span=model/conv2d_4/Conv2D:0:0 */, %v_param_10: Tensor[(32), int32] /* ty=Tensor[(32), int32] span=model/batch_normalization_4/FusedBatchNormV3;model/conv2d_4/BiasAdd/ReadVariableOp/resource;model/conv2d_4/BiasAdd:0:0 */, %v_param_17: Tensor[(1, 1, 32, 64), int8] /* ty=Tensor[(1, 1, 32, 64), int8] span=model/conv2d_8/Conv2D:0:0 */, %v_param_18: Tensor[(64), int32] /* ty=Tensor[(64), int32] span=model/conv2d_8/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_8/BiasAdd/ReadVariableOp/resource:0:0 */, %v_param_13: Tensor[(3, 3, 32, 64), int8] /* ty=Tensor[(3, 3, 32, 64), int8] span=model/conv2d_6/Conv2D:0:0 */, %v_param_14: Tensor[(64), int32] /* ty=Tensor[(64), int32] span=model/activation_5/Relu;model/batch_normalization_5/FusedBatchNormV3;model/conv2d_6/BiasAdd/ReadVariableOp/resource;model/conv2d_6/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_6/Conv2D:0:0 */, %v_param_15: Tensor[(3, 3, 64, 64), int8] /* ty=Tensor[(3, 3, 64, 64), int8] span=model/conv2d_7/Conv2D:0:0 */, %v_param_16: Tensor[(64), int32] /* ty=Tensor[(64), int32] span=model/batch_normalization_6/FusedBatchNormV3;model/conv2d_7/BiasAdd/ReadVariableOp/resource;model/conv2d_7/BiasAdd:0:0 */, %v_param_19: Tensor[(10, 64), int8] /* ty=Tensor[(10, 64), int8] span=model/dense/MatMul:0:0 */, %v_param_20: Tensor[(10), int32] /* ty=Tensor[(10), int32] span=model/dense/BiasAdd/ReadVariableOp/resource:0:0 */, output_tensor_names=["Identity_int8"]) -> Tensor[(1, 10), int8] {
  %0 = qnn.conv2d(%input_1_int8, %v_param_1, -128 /* ty=int32 span=model/activation/Relu;model/batch_normalization/FusedBatchNormV3;model/conv2d/BiasAdd/ReadVariableOp/resource;model/conv2d/BiasAdd;model/conv2d_2/Conv2D;model/conv2d/Conv2D1:0:0 */, 0 /* ty=int32 span=model/activation/Relu;model/batch_normalization/FusedBatchNormV3;model/conv2d/BiasAdd/ReadVariableOp/resource;model/conv2d/BiasAdd;model/conv2d_2/Conv2D;model/conv2d/Conv2D1:0:0 */, 1f /* ty=float32 span=model/activation/Relu;model/batch_normalization/FusedBatchNormV3;model/conv2d/BiasAdd/ReadVariableOp/resource;model/conv2d/BiasAdd;model/conv2d_2/Conv2D;model/conv2d/Conv2D1:0:0 */, meta[relay.Constant][0] /* ty=Tensor[(16), float32] span=model/activation/Relu;model/batch_normalization/FusedBatchNormV3;model/conv2d/BiasAdd/ReadVariableOp/resource;model/conv2d/BiasAdd;model/conv2d_2/Conv2D;model/conv2d/Conv2D1:0:0 */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 32, 32, 16), int32] span=model/activation/Relu;model/batch_normalization/FusedBatchNormV3;model/conv2d/BiasAdd/ReadVariableOp/resource;model/conv2d/BiasAdd;model/conv2d_2/Conv2D;model/conv2d/Conv2D1:0:0 */;
  %1 = nn.bias_add(%0, %v_param_2, axis=3) /* ty=Tensor[(1, 32, 32, 16), int32] span=model/activation/Relu;model/batch_normalization/FusedBatchNormV3;model/conv2d/BiasAdd/ReadVariableOp/resource;model/conv2d/BiasAdd;model/conv2d_2/Conv2D;model/conv2d/Conv2D1:0:0 */;
  %2 = qnn.requantize(%1, meta[relay.Constant][1] /* ty=Tensor[(16), float32] span=model/activation/Relu;model/batch_normalization/FusedBatchNormV3;model/conv2d/BiasAdd/ReadVariableOp/resource;model/conv2d/BiasAdd;model/conv2d_2/Conv2D;model/conv2d/Conv2D1:0:0 */, 0 /* ty=int32 span=model/activation/Relu;model/batch_normalization/FusedBatchNormV3;model/conv2d/BiasAdd/ReadVariableOp/resource;model/conv2d/BiasAdd;model/conv2d_2/Conv2D;model/conv2d/Conv2D1:0:0 */, 0.0393936f /* ty=float32 span=model/activation/Relu;model/batch_normalization/FusedBatchNormV3;model/conv2d/BiasAdd/ReadVariableOp/resource;model/conv2d/BiasAdd;model/conv2d_2/Conv2D;model/conv2d/Conv2D1:0:0 */, -128 /* ty=int32 span=model/activation/Relu;model/batch_normalization/FusedBatchNormV3;model/conv2d/BiasAdd/ReadVariableOp/resource;model/conv2d/BiasAdd;model/conv2d_2/Conv2D;model/conv2d/Conv2D1:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="int8") /* ty=Tensor[(1, 32, 32, 16), int8] span=model/activation/Relu;model/batch_normalization/FusedBatchNormV3;model/conv2d/BiasAdd/ReadVariableOp/resource;model/conv2d/BiasAdd;model/conv2d_2/Conv2D;model/conv2d/Conv2D1:0:0 */;
  %3 = clip(%2, a_min=-128f, a_max=127f) /* ty=Tensor[(1, 32, 32, 16), int8] span=model/activation/Relu;model/batch_normalization/FusedBatchNormV3;model/conv2d/BiasAdd/ReadVariableOp/resource;model/conv2d/BiasAdd;model/conv2d_2/Conv2D;model/conv2d/Conv2D1:0:0 */;
  %4 = qnn.conv2d(%3, %v_param_3, -128 /* ty=int32 span=model/activation_1/Relu;model/batch_normalization_1/FusedBatchNormV3;model/conv2d_1/BiasAdd/ReadVariableOp/resource;model/conv2d_1/BiasAdd;model/conv2d_2/Conv2D;model/conv2d_1/Conv2D1:0:0 */, 0 /* ty=int32 span=model/activation_1/Relu;model/batch_normalization_1/FusedBatchNormV3;model/conv2d_1/BiasAdd/ReadVariableOp/resource;model/conv2d_1/BiasAdd;model/conv2d_2/Conv2D;model/conv2d_1/Conv2D1:0:0 */, 0.0393936f /* ty=float32 span=model/activation_1/Relu;model/batch_normalization_1/FusedBatchNormV3;model/conv2d_1/BiasAdd/ReadVariableOp/resource;model/conv2d_1/BiasAdd;model/conv2d_2/Conv2D;model/conv2d_1/Conv2D1:0:0 */, meta[relay.Constant][2] /* ty=Tensor[(16), float32] span=model/activation_1/Relu;model/batch_normalization_1/FusedBatchNormV3;model/conv2d_1/BiasAdd/ReadVariableOp/resource;model/conv2d_1/BiasAdd;model/conv2d_2/Conv2D;model/conv2d_1/Conv2D1:0:0 */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 32, 32, 16), int32] span=model/activation_1/Relu;model/batch_normalization_1/FusedBatchNormV3;model/conv2d_1/BiasAdd/ReadVariableOp/resource;model/conv2d_1/BiasAdd;model/conv2d_2/Conv2D;model/conv2d_1/Conv2D1:0:0 */;
  %5 = nn.bias_add(%4, %v_param_4, axis=3) /* ty=Tensor[(1, 32, 32, 16), int32] span=model/activation_1/Relu;model/batch_normalization_1/FusedBatchNormV3;model/conv2d_1/BiasAdd/ReadVariableOp/resource;model/conv2d_1/BiasAdd;model/conv2d_2/Conv2D;model/conv2d_1/Conv2D1:0:0 */;
  %6 = qnn.requantize(%5, meta[relay.Constant][3] /* ty=Tensor[(16), float32] span=model/activation_1/Relu;model/batch_normalization_1/FusedBatchNormV3;model/conv2d_1/BiasAdd/ReadVariableOp/resource;model/conv2d_1/BiasAdd;model/conv2d_2/Conv2D;model/conv2d_1/Conv2D1:0:0 */, 0 /* ty=int32 span=model/activation_1/Relu;model/batch_normalization_1/FusedBatchNormV3;model/conv2d_1/BiasAdd/ReadVariableOp/resource;model/conv2d_1/BiasAdd;model/conv2d_2/Conv2D;model/conv2d_1/Conv2D1:0:0 */, 0.0762932f /* ty=float32 span=model/activation_1/Relu;model/batch_normalization_1/FusedBatchNormV3;model/conv2d_1/BiasAdd/ReadVariableOp/resource;model/conv2d_1/BiasAdd;model/conv2d_2/Conv2D;model/conv2d_1/Conv2D1:0:0 */, -128 /* ty=int32 span=model/activation_1/Relu;model/batch_normalization_1/FusedBatchNormV3;model/conv2d_1/BiasAdd/ReadVariableOp/resource;model/conv2d_1/BiasAdd;model/conv2d_2/Conv2D;model/conv2d_1/Conv2D1:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="int8") /* ty=Tensor[(1, 32, 32, 16), int8] span=model/activation_1/Relu;model/batch_normalization_1/FusedBatchNormV3;model/conv2d_1/BiasAdd/ReadVariableOp/resource;model/conv2d_1/BiasAdd;model/conv2d_2/Conv2D;model/conv2d_1/Conv2D1:0:0 */;
  %7 = clip(%6, a_min=-128f, a_max=127f) /* ty=Tensor[(1, 32, 32, 16), int8] span=model/activation_1/Relu;model/batch_normalization_1/FusedBatchNormV3;model/conv2d_1/BiasAdd/ReadVariableOp/resource;model/conv2d_1/BiasAdd;model/conv2d_2/Conv2D;model/conv2d_1/Conv2D1:0:0 */;
  %8 = qnn.conv2d(%7, %v_param_5, -128 /* ty=int32 span=model/batch_normalization_2/FusedBatchNormV3;model/conv2d_2/BiasAdd/ReadVariableOp/resource;model/conv2d_2/BiasAdd;model/conv2d_2/Conv2D:0:0 */, 0 /* ty=int32 span=model/batch_normalization_2/FusedBatchNormV3;model/conv2d_2/BiasAdd/ReadVariableOp/resource;model/conv2d_2/BiasAdd;model/conv2d_2/Conv2D:0:0 */, 0.0762932f /* ty=float32 span=model/batch_normalization_2/FusedBatchNormV3;model/conv2d_2/BiasAdd/ReadVariableOp/resource;model/conv2d_2/BiasAdd;model/conv2d_2/Conv2D:0:0 */, meta[relay.Constant][4] /* ty=Tensor[(16), float32] span=model/batch_normalization_2/FusedBatchNormV3;model/conv2d_2/BiasAdd/ReadVariableOp/resource;model/conv2d_2/BiasAdd;model/conv2d_2/Conv2D:0:0 */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 32, 32, 16), int32] span=model/batch_normalization_2/FusedBatchNormV3;model/conv2d_2/BiasAdd/ReadVariableOp/resource;model/conv2d_2/BiasAdd;model/conv2d_2/Conv2D:0:0 */;
  %9 = nn.bias_add(%8, %v_param_6, axis=3) /* ty=Tensor[(1, 32, 32, 16), int32] span=model/batch_normalization_2/FusedBatchNormV3;model/conv2d_2/BiasAdd/ReadVariableOp/resource;model/conv2d_2/BiasAdd;model/conv2d_2/Conv2D:0:0 */;
  %10 = qnn.requantize(%9, meta[relay.Constant][5] /* ty=Tensor[(16), float32] span=model/batch_normalization_2/FusedBatchNormV3;model/conv2d_2/BiasAdd/ReadVariableOp/resource;model/conv2d_2/BiasAdd;model/conv2d_2/Conv2D:0:0 */, 0 /* ty=int32 span=model/batch_normalization_2/FusedBatchNormV3;model/conv2d_2/BiasAdd/ReadVariableOp/resource;model/conv2d_2/BiasAdd;model/conv2d_2/Conv2D:0:0 */, 0.104195f /* ty=float32 span=model/batch_normalization_2/FusedBatchNormV3;model/conv2d_2/BiasAdd/ReadVariableOp/resource;model/conv2d_2/BiasAdd;model/conv2d_2/Conv2D:0:0 */, 4 /* ty=int32 span=model/batch_normalization_2/FusedBatchNormV3;model/conv2d_2/BiasAdd/ReadVariableOp/resource;model/conv2d_2/BiasAdd;model/conv2d_2/Conv2D:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="int8") /* ty=Tensor[(1, 32, 32, 16), int8] span=model/batch_normalization_2/FusedBatchNormV3;model/conv2d_2/BiasAdd/ReadVariableOp/resource;model/conv2d_2/BiasAdd;model/conv2d_2/Conv2D:0:0 */;
  %11 = qnn.add(%3, %10, 0.0393936f /* ty=float32 span=model/activation_2/Relu;model/add/add:0:0 */, -128 /* ty=int32 span=model/activation_2/Relu;model/add/add:0:0 */, 0.104195f /* ty=float32 span=model/activation_2/Relu;model/add/add:0:0 */, 4 /* ty=int32 span=model/activation_2/Relu;model/add/add:0:0 */, 0.0509457f /* ty=float32 span=model/activation_2/Relu;model/add/add:0:0 */, -128 /* ty=int32 span=model/activation_2/Relu;model/add/add:0:0 */) /* ty=Tensor[(1, 32, 32, 16), int8] span=model/activation_2/Relu;model/add/add:0:0 */;
  %12 = clip(%11, a_min=-128f, a_max=127f) /* ty=Tensor[(1, 32, 32, 16), int8] span=model/activation_2/Relu;model/add/add:0:0 */;
  %13 = qnn.conv2d(%12, %v_param_11, -128 /* ty=int32 span=model/conv2d_5/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_5/BiasAdd/ReadVariableOp/resource1:0:0 */, 0 /* ty=int32 span=model/conv2d_5/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_5/BiasAdd/ReadVariableOp/resource1:0:0 */, 0.0509457f /* ty=float32 span=model/conv2d_5/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_5/BiasAdd/ReadVariableOp/resource1:0:0 */, meta[relay.Constant][6] /* ty=Tensor[(32), float32] span=model/conv2d_5/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_5/BiasAdd/ReadVariableOp/resource1:0:0 */, strides=[2, 2], padding=[0, 0, 0, 0], channels=32, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 16, 16, 32), int32] span=model/conv2d_5/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_5/BiasAdd/ReadVariableOp/resource1:0:0 */;
  %14 = nn.bias_add(%13, %v_param_12, axis=3) /* ty=Tensor[(1, 16, 16, 32), int32] span=model/conv2d_5/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_5/BiasAdd/ReadVariableOp/resource1:0:0 */;
  %15 = qnn.conv2d(%12, %v_param_7, -128 /* ty=int32 span=model/activation_3/Relu;model/batch_normalization_3/FusedBatchNormV3;model/conv2d_3/BiasAdd/ReadVariableOp/resource;model/conv2d_3/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_3/Conv2D1:0:0 */, 0 /* ty=int32 span=model/activation_3/Relu;model/batch_normalization_3/FusedBatchNormV3;model/conv2d_3/BiasAdd/ReadVariableOp/resource;model/conv2d_3/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_3/Conv2D1:0:0 */, 0.0509457f /* ty=float32 span=model/activation_3/Relu;model/batch_normalization_3/FusedBatchNormV3;model/conv2d_3/BiasAdd/ReadVariableOp/resource;model/conv2d_3/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_3/Conv2D1:0:0 */, meta[relay.Constant][8] /* ty=Tensor[(32), float32] span=model/activation_3/Relu;model/batch_normalization_3/FusedBatchNormV3;model/conv2d_3/BiasAdd/ReadVariableOp/resource;model/conv2d_3/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_3/Conv2D1:0:0 */, strides=[2, 2], padding=[0, 0, 1, 1], channels=32, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 16, 16, 32), int32] span=model/activation_3/Relu;model/batch_normalization_3/FusedBatchNormV3;model/conv2d_3/BiasAdd/ReadVariableOp/resource;model/conv2d_3/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_3/Conv2D1:0:0 */;
  %16 = nn.bias_add(%15, %v_param_8, axis=3) /* ty=Tensor[(1, 16, 16, 32), int32] span=model/activation_3/Relu;model/batch_normalization_3/FusedBatchNormV3;model/conv2d_3/BiasAdd/ReadVariableOp/resource;model/conv2d_3/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_3/Conv2D1:0:0 */;
  %17 = qnn.requantize(%16, meta[relay.Constant][9] /* ty=Tensor[(32), float32] span=model/activation_3/Relu;model/batch_normalization_3/FusedBatchNormV3;model/conv2d_3/BiasAdd/ReadVariableOp/resource;model/conv2d_3/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_3/Conv2D1:0:0 */, 0 /* ty=int32 span=model/activation_3/Relu;model/batch_normalization_3/FusedBatchNormV3;model/conv2d_3/BiasAdd/ReadVariableOp/resource;model/conv2d_3/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_3/Conv2D1:0:0 */, 0.0456728f /* ty=float32 span=model/activation_3/Relu;model/batch_normalization_3/FusedBatchNormV3;model/conv2d_3/BiasAdd/ReadVariableOp/resource;model/conv2d_3/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_3/Conv2D1:0:0 */, -128 /* ty=int32 span=model/activation_3/Relu;model/batch_normalization_3/FusedBatchNormV3;model/conv2d_3/BiasAdd/ReadVariableOp/resource;model/conv2d_3/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_3/Conv2D1:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="int8") /* ty=Tensor[(1, 16, 16, 32), int8] span=model/activation_3/Relu;model/batch_normalization_3/FusedBatchNormV3;model/conv2d_3/BiasAdd/ReadVariableOp/resource;model/conv2d_3/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_3/Conv2D1:0:0 */;
  %18 = clip(%17, a_min=-128f, a_max=127f) /* ty=Tensor[(1, 16, 16, 32), int8] span=model/activation_3/Relu;model/batch_normalization_3/FusedBatchNormV3;model/conv2d_3/BiasAdd/ReadVariableOp/resource;model/conv2d_3/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_3/Conv2D1:0:0 */;
  %19 = qnn.conv2d(%18, %v_param_9, -128 /* ty=int32 span=model/batch_normalization_4/FusedBatchNormV3;model/conv2d_4/BiasAdd/ReadVariableOp/resource;model/conv2d_4/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_4/Conv2D:0:0 */, 0 /* ty=int32 span=model/batch_normalization_4/FusedBatchNormV3;model/conv2d_4/BiasAdd/ReadVariableOp/resource;model/conv2d_4/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_4/Conv2D:0:0 */, 0.0456728f /* ty=float32 span=model/batch_normalization_4/FusedBatchNormV3;model/conv2d_4/BiasAdd/ReadVariableOp/resource;model/conv2d_4/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_4/Conv2D:0:0 */, meta[relay.Constant][10] /* ty=Tensor[(32), float32] span=model/batch_normalization_4/FusedBatchNormV3;model/conv2d_4/BiasAdd/ReadVariableOp/resource;model/conv2d_4/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_4/Conv2D:0:0 */, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 16, 16, 32), int32] span=model/batch_normalization_4/FusedBatchNormV3;model/conv2d_4/BiasAdd/ReadVariableOp/resource;model/conv2d_4/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_4/Conv2D:0:0 */;
  %20 = nn.bias_add(%19, %v_param_10, axis=3) /* ty=Tensor[(1, 16, 16, 32), int32] span=model/batch_normalization_4/FusedBatchNormV3;model/conv2d_4/BiasAdd/ReadVariableOp/resource;model/conv2d_4/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_4/Conv2D:0:0 */;
  %21 = qnn.requantize(%14, meta[relay.Constant][7] /* ty=Tensor[(32), float32] span=model/conv2d_5/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_5/BiasAdd/ReadVariableOp/resource1:0:0 */, 0 /* ty=int32 span=model/conv2d_5/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_5/BiasAdd/ReadVariableOp/resource1:0:0 */, 0.0447614f /* ty=float32 span=model/conv2d_5/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_5/BiasAdd/ReadVariableOp/resource1:0:0 */, -17 /* ty=int32 span=model/conv2d_5/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_5/BiasAdd/ReadVariableOp/resource1:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="int8") /* ty=Tensor[(1, 16, 16, 32), int8] span=model/conv2d_5/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_5/BiasAdd/ReadVariableOp/resource1:0:0 */;
  %22 = qnn.requantize(%20, meta[relay.Constant][11] /* ty=Tensor[(32), float32] span=model/batch_normalization_4/FusedBatchNormV3;model/conv2d_4/BiasAdd/ReadVariableOp/resource;model/conv2d_4/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_4/Conv2D:0:0 */, 0 /* ty=int32 span=model/batch_normalization_4/FusedBatchNormV3;model/conv2d_4/BiasAdd/ReadVariableOp/resource;model/conv2d_4/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_4/Conv2D:0:0 */, 0.113119f /* ty=float32 span=model/batch_normalization_4/FusedBatchNormV3;model/conv2d_4/BiasAdd/ReadVariableOp/resource;model/conv2d_4/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_4/Conv2D:0:0 */, 4 /* ty=int32 span=model/batch_normalization_4/FusedBatchNormV3;model/conv2d_4/BiasAdd/ReadVariableOp/resource;model/conv2d_4/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_4/Conv2D:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="int8") /* ty=Tensor[(1, 16, 16, 32), int8] span=model/batch_normalization_4/FusedBatchNormV3;model/conv2d_4/BiasAdd/ReadVariableOp/resource;model/conv2d_4/BiasAdd;model/conv2d_5/Conv2D;model/conv2d_4/Conv2D:0:0 */;
  %23 = qnn.add(%21, %22, 0.0447614f /* ty=float32 span=model/activation_4/Relu;model/add_1/add:0:0 */, -17 /* ty=int32 span=model/activation_4/Relu;model/add_1/add:0:0 */, 0.113119f /* ty=float32 span=model/activation_4/Relu;model/add_1/add:0:0 */, 4 /* ty=int32 span=model/activation_4/Relu;model/add_1/add:0:0 */, 0.0532362f /* ty=float32 span=model/activation_4/Relu;model/add_1/add:0:0 */, -128 /* ty=int32 span=model/activation_4/Relu;model/add_1/add:0:0 */) /* ty=Tensor[(1, 16, 16, 32), int8] span=model/activation_4/Relu;model/add_1/add:0:0 */;
  %24 = clip(%23, a_min=-128f, a_max=127f) /* ty=Tensor[(1, 16, 16, 32), int8] span=model/activation_4/Relu;model/add_1/add:0:0 */;
  %25 = qnn.conv2d(%24, %v_param_17, -128 /* ty=int32 span=model/conv2d_8/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_8/BiasAdd/ReadVariableOp/resource1:0:0 */, 0 /* ty=int32 span=model/conv2d_8/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_8/BiasAdd/ReadVariableOp/resource1:0:0 */, 0.0532362f /* ty=float32 span=model/conv2d_8/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_8/BiasAdd/ReadVariableOp/resource1:0:0 */, meta[relay.Constant][12] /* ty=Tensor[(64), float32] span=model/conv2d_8/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_8/BiasAdd/ReadVariableOp/resource1:0:0 */, strides=[2, 2], padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 8, 8, 64), int32] span=model/conv2d_8/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_8/BiasAdd/ReadVariableOp/resource1:0:0 */;
  %26 = nn.bias_add(%25, %v_param_18, axis=3) /* ty=Tensor[(1, 8, 8, 64), int32] span=model/conv2d_8/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_8/BiasAdd/ReadVariableOp/resource1:0:0 */;
  %27 = qnn.conv2d(%24, %v_param_13, -128 /* ty=int32 span=model/activation_5/Relu;model/batch_normalization_5/FusedBatchNormV3;model/conv2d_6/BiasAdd/ReadVariableOp/resource;model/conv2d_6/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_6/Conv2D1:0:0 */, 0 /* ty=int32 span=model/activation_5/Relu;model/batch_normalization_5/FusedBatchNormV3;model/conv2d_6/BiasAdd/ReadVariableOp/resource;model/conv2d_6/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_6/Conv2D1:0:0 */, 0.0532362f /* ty=float32 span=model/activation_5/Relu;model/batch_normalization_5/FusedBatchNormV3;model/conv2d_6/BiasAdd/ReadVariableOp/resource;model/conv2d_6/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_6/Conv2D1:0:0 */, meta[relay.Constant][14] /* ty=Tensor[(64), float32] span=model/activation_5/Relu;model/batch_normalization_5/FusedBatchNormV3;model/conv2d_6/BiasAdd/ReadVariableOp/resource;model/conv2d_6/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_6/Conv2D1:0:0 */, strides=[2, 2], padding=[0, 0, 1, 1], channels=64, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 8, 8, 64), int32] span=model/activation_5/Relu;model/batch_normalization_5/FusedBatchNormV3;model/conv2d_6/BiasAdd/ReadVariableOp/resource;model/conv2d_6/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_6/Conv2D1:0:0 */;
  %28 = nn.bias_add(%27, %v_param_14, axis=3) /* ty=Tensor[(1, 8, 8, 64), int32] span=model/activation_5/Relu;model/batch_normalization_5/FusedBatchNormV3;model/conv2d_6/BiasAdd/ReadVariableOp/resource;model/conv2d_6/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_6/Conv2D1:0:0 */;
  %29 = qnn.requantize(%28, meta[relay.Constant][15] /* ty=Tensor[(64), float32] span=model/activation_5/Relu;model/batch_normalization_5/FusedBatchNormV3;model/conv2d_6/BiasAdd/ReadVariableOp/resource;model/conv2d_6/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_6/Conv2D1:0:0 */, 0 /* ty=int32 span=model/activation_5/Relu;model/batch_normalization_5/FusedBatchNormV3;model/conv2d_6/BiasAdd/ReadVariableOp/resource;model/conv2d_6/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_6/Conv2D1:0:0 */, 0.0284502f /* ty=float32 span=model/activation_5/Relu;model/batch_normalization_5/FusedBatchNormV3;model/conv2d_6/BiasAdd/ReadVariableOp/resource;model/conv2d_6/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_6/Conv2D1:0:0 */, -128 /* ty=int32 span=model/activation_5/Relu;model/batch_normalization_5/FusedBatchNormV3;model/conv2d_6/BiasAdd/ReadVariableOp/resource;model/conv2d_6/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_6/Conv2D1:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="int8") /* ty=Tensor[(1, 8, 8, 64), int8] span=model/activation_5/Relu;model/batch_normalization_5/FusedBatchNormV3;model/conv2d_6/BiasAdd/ReadVariableOp/resource;model/conv2d_6/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_6/Conv2D1:0:0 */;
  %30 = clip(%29, a_min=-128f, a_max=127f) /* ty=Tensor[(1, 8, 8, 64), int8] span=model/activation_5/Relu;model/batch_normalization_5/FusedBatchNormV3;model/conv2d_6/BiasAdd/ReadVariableOp/resource;model/conv2d_6/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_6/Conv2D1:0:0 */;
  %31 = qnn.conv2d(%30, %v_param_15, -128 /* ty=int32 span=model/batch_normalization_6/FusedBatchNormV3;model/conv2d_7/BiasAdd/ReadVariableOp/resource;model/conv2d_7/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_7/Conv2D:0:0 */, 0 /* ty=int32 span=model/batch_normalization_6/FusedBatchNormV3;model/conv2d_7/BiasAdd/ReadVariableOp/resource;model/conv2d_7/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_7/Conv2D:0:0 */, 0.0284502f /* ty=float32 span=model/batch_normalization_6/FusedBatchNormV3;model/conv2d_7/BiasAdd/ReadVariableOp/resource;model/conv2d_7/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_7/Conv2D:0:0 */, meta[relay.Constant][16] /* ty=Tensor[(64), float32] span=model/batch_normalization_6/FusedBatchNormV3;model/conv2d_7/BiasAdd/ReadVariableOp/resource;model/conv2d_7/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_7/Conv2D:0:0 */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO", out_dtype="int32") /* ty=Tensor[(1, 8, 8, 64), int32] span=model/batch_normalization_6/FusedBatchNormV3;model/conv2d_7/BiasAdd/ReadVariableOp/resource;model/conv2d_7/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_7/Conv2D:0:0 */;
  %32 = nn.bias_add(%31, %v_param_16, axis=3) /* ty=Tensor[(1, 8, 8, 64), int32] span=model/batch_normalization_6/FusedBatchNormV3;model/conv2d_7/BiasAdd/ReadVariableOp/resource;model/conv2d_7/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_7/Conv2D:0:0 */;
  %33 = qnn.requantize(%26, meta[relay.Constant][13] /* ty=Tensor[(64), float32] span=model/conv2d_8/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_8/BiasAdd/ReadVariableOp/resource1:0:0 */, 0 /* ty=int32 span=model/conv2d_8/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_8/BiasAdd/ReadVariableOp/resource1:0:0 */, 0.0838583f /* ty=float32 span=model/conv2d_8/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_8/BiasAdd/ReadVariableOp/resource1:0:0 */, 38 /* ty=int32 span=model/conv2d_8/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_8/BiasAdd/ReadVariableOp/resource1:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="int8") /* ty=Tensor[(1, 8, 8, 64), int8] span=model/conv2d_8/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_8/BiasAdd/ReadVariableOp/resource1:0:0 */;
  %34 = qnn.requantize(%32, meta[relay.Constant][17] /* ty=Tensor[(64), float32] span=model/batch_normalization_6/FusedBatchNormV3;model/conv2d_7/BiasAdd/ReadVariableOp/resource;model/conv2d_7/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_7/Conv2D:0:0 */, 0 /* ty=int32 span=model/batch_normalization_6/FusedBatchNormV3;model/conv2d_7/BiasAdd/ReadVariableOp/resource;model/conv2d_7/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_7/Conv2D:0:0 */, 0.217244f /* ty=float32 span=model/batch_normalization_6/FusedBatchNormV3;model/conv2d_7/BiasAdd/ReadVariableOp/resource;model/conv2d_7/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_7/Conv2D:0:0 */, -2 /* ty=int32 span=model/batch_normalization_6/FusedBatchNormV3;model/conv2d_7/BiasAdd/ReadVariableOp/resource;model/conv2d_7/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_7/Conv2D:0:0 */, axis=3, rounding="UPWARD", compute_dtype="int64", out_dtype="int8") /* ty=Tensor[(1, 8, 8, 64), int8] span=model/batch_normalization_6/FusedBatchNormV3;model/conv2d_7/BiasAdd/ReadVariableOp/resource;model/conv2d_7/BiasAdd;model/conv2d_8/Conv2D;model/conv2d_7/Conv2D:0:0 */;
  %35 = qnn.add(%33, %34, 0.0838583f /* ty=float32 span=model/activation_6/Relu;model/add_2/add:0:0 */, 38 /* ty=int32 span=model/activation_6/Relu;model/add_2/add:0:0 */, 0.217244f /* ty=float32 span=model/activation_6/Relu;model/add_2/add:0:0 */, -2 /* ty=int32 span=model/activation_6/Relu;model/add_2/add:0:0 */, 0.127069f /* ty=float32 span=model/activation_6/Relu;model/add_2/add:0:0 */, -128 /* ty=int32 span=model/activation_6/Relu;model/add_2/add:0:0 */) /* ty=Tensor[(1, 8, 8, 64), int8] span=model/activation_6/Relu;model/add_2/add:0:0 */;
  %36 = clip(%35, a_min=-128f, a_max=127f) /* ty=Tensor[(1, 8, 8, 64), int8] span=model/activation_6/Relu;model/add_2/add:0:0 */;
  %37 = cast(%36, dtype="int32") /* ty=Tensor[(1, 8, 8, 64), int32] span=model/average_pooling2d/AvgPool:0:0 */;
  %38 = nn.avg_pool2d(%37, pool_size=[8, 8], strides=[8, 8], padding=[0, 0, 0, 0], layout="NHWC") /* ty=Tensor[(1, 1, 1, 64), int32] span=model/average_pooling2d/AvgPool:0:0 */;
  %39 = cast(%38, dtype="int8") /* ty=Tensor[(1, 1, 1, 64), int8] span=model/average_pooling2d/AvgPool:0:0 */;
  %40 = reshape(%39, newshape=[-1, 64]) /* ty=Tensor[(1, 64), int8] span=model/flatten/Reshape:0:0 */;
  %41 = reshape(%40, newshape=[-1, 64]) /* ty=Tensor[(1, 64), int8] span=model/dense/MatMul;model/dense/BiasAdd:0:0 */;
  %42 = qnn.dense(%41, %v_param_19, -128 /* ty=int32 span=model/dense/MatMul;model/dense/BiasAdd:0:0 */, 0 /* ty=int32 span=model/dense/MatMul;model/dense/BiasAdd:0:0 */, 0.127069f /* ty=float32 span=model/dense/MatMul;model/dense/BiasAdd:0:0 */, 0.0305544f /* ty=float32 span=model/dense/MatMul;model/dense/BiasAdd:0:0 */, units=10, out_dtype="int32") /* ty=Tensor[(1, 10), int32] span=model/dense/MatMul;model/dense/BiasAdd:0:0 */;
  %43 = nn.bias_add(%42, %v_param_20) /* ty=Tensor[(1, 10), int32] span=model/dense/MatMul;model/dense/BiasAdd:0:0 */;
  %44 = qnn.requantize(%43, 0.00388252f /* ty=float32 span=model/dense/MatMul;model/dense/BiasAdd:0:0 */, 0 /* ty=int32 span=model/dense/MatMul;model/dense/BiasAdd:0:0 */, 0.171854f /* ty=float32 span=model/dense/MatMul;model/dense/BiasAdd:0:0 */, 24 /* ty=int32 span=model/dense/MatMul;model/dense/BiasAdd:0:0 */, rounding="UPWARD", compute_dtype="int64", out_dtype="int8") /* ty=Tensor[(1, 10), int8] span=model/dense/MatMul;model/dense/BiasAdd:0:0 */;
  %45 = qnn.dequantize(%44, 0.171854f /* ty=float32 span=Identity_int8:0:0 */, 24 /* ty=int32 span=Identity_int8:0:0 */, out_dtype="float32") /* ty=Tensor[(1, 10), float32] span=Identity_int8:0:0 */;
  %46 = nn.softmax(%45) /* ty=Tensor[(1, 10), float32] span=Identity_int8:0:0 */;
  qnn.quantize(%46, 0.00390625f /* ty=float32 span=Identity_int8:0:0 */, -128 /* ty=int32 span=Identity_int8:0:0 */, out_dtype="int8") /* ty=Tensor[(1, 10), int8] span=Identity_int8:0:0 */
}

